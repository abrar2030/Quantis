"""
Dataset service for data management operations
"""
import os
import json
import hashlib
from typing import List, Optional, Dict, Any
from sqlalchemy.orm import Session
from sqlalchemy import and_
import pandas as pd

from .. import models_enhanced as models
from ..database_enhanced import EncryptionManager, DataRetentionManager
from ..config import get_settings

settings = get_settings()

class DatasetService:
    def __init__(self, db: Session):
        self.db = db

    def create_dataset_record(self, name: str, description: str, owner_id: int,
                              file_path: Optional[str] = None, file_size: Optional[int] = None,
                              file_hash: Optional[str] = None, columns_info: Optional[Dict] = None,
                              row_count: Optional[int] = None, status: models.DatasetStatus = models.DatasetStatus.UPLOADING,
                              tags: Optional[List[str]] = None, source: Optional[str] = None,
                              frequency: Optional[str] = None) -> models.Dataset:
        """Create a new dataset record in the database."""
        # Validate owner exists
        owner = self.db.query(models.User).filter(models.User.id == owner_id).first()
        if not owner:
            raise ValueError("Owner not found")
        
        dataset = models.Dataset(
            name=name,
            description=description,
            owner_id=owner_id,
            file_path=file_path,
            file_size=file_size,
            file_hash=file_hash,
            columns_info=columns_info,
            row_count=row_count,
            status=status,
            tags=tags or [],
            source=source,
            frequency=frequency
        )
        
        self.db.add(dataset)
        self.db.commit()
        self.db.refresh(dataset)
        return dataset

    def get_dataset_by_id(self, dataset_id: int) -> Optional[models.Dataset]:
        """Get dataset by ID"""
        return self.db.query(models.Dataset).filter(
            and_(models.Dataset.id == dataset_id, models.Dataset.is_deleted == False)
        ).first()

    def get_datasets_by_owner(self, owner_id: int, skip: int = 0, limit: int = 100) -> List[models.Dataset]:
        """Get datasets by owner"""
        return self.db.query(models.Dataset).filter(
            and_(models.Dataset.owner_id == owner_id, models.Dataset.is_deleted == False)
        ).offset(skip).limit(limit).all()

    def get_all_datasets(self, skip: int = 0, limit: int = 100) -> List[models.Dataset]:
        """Get all datasets (admin only)"""
        return self.db.query(models.Dataset).filter(models.Dataset.is_deleted == False).offset(skip).limit(limit).all()

    def update_dataset_record(self, dataset_id: int, **kwargs) -> Optional[models.Dataset]:
        """Update dataset information"""
        dataset = self.get_dataset_by_id(dataset_id)
        if not dataset:
            return None
        
        for key, value in kwargs.items():
            if hasattr(dataset, key) and key not in ["id", "owner_id", "created_at", "file_path", "file_size", "file_hash", "columns_info", "row_count", "status"]:
                setattr(dataset, key, value)
        
        self.db.commit()
        self.db.refresh(dataset)
        return dataset

    def soft_delete_dataset(self, dataset_id: int, deleted_by_id: int) -> bool:
        """Soft delete dataset"""
        dataset = self.get_dataset_by_id(dataset_id)
        if not dataset:
            return False
        
        dataset.is_deleted = True
        dataset.deleted_at = datetime.utcnow()
        dataset.deleted_by_id = deleted_by_id
        self.db.commit()
        return True

    def load_dataset_data(self, file_path: str, encryption_manager: EncryptionManager) -> Optional[pd.DataFrame]:
        """Load dataset data as pandas DataFrame, with decryption if enabled"""
        if not os.path.exists(file_path):
            return None
        
        try:
            if settings.compliance.enable_data_encryption:
                with open(file_path, "r", encoding="utf-8") as f:
                    encrypted_content = f.read()
                decrypted_content = encryption_manager.decrypt(encrypted_content)
                # Save to a temporary file for pandas to read
                with tempfile.NamedTemporaryFile(delete=False, suffix=".tmp") as tmp_file:
                    tmp_file.write(decrypted_content.encode("utf-8"))
                    tmp_file_path = tmp_file.name
            else:
                tmp_file_path = file_path

            if tmp_file_path.endswith(".csv") or tmp_file_path.endswith(".tmp"):
                return pd.read_csv(tmp_file_path)
            elif tmp_file_path.endswith(".json"):
                return pd.read_json(tmp_file_path)
            elif tmp_file_path.endswith(tuple(settings.allowed_file_types)):
                # Handle other allowed types if necessary
                if tmp_file_path.endswith(('.xlsx', '.xls')):
                    return pd.read_excel(tmp_file_path)
                else:
                    return None # Or raise an error for unsupported format
            else:
                return None
        except Exception as e:
            logger.error(f"Error loading or decrypting dataset from {file_path}: {e}")
            return None
        finally:
            if settings.compliance.enable_data_encryption and os.path.exists(tmp_file_path) and tmp_file_path != file_path:
                os.remove(tmp_file_path)

    def calculate_dataset_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """Calculate statistical information about a pandas DataFrame"""
        stats = {
            "shape": data.shape,
            "columns": list(data.columns),
            "dtypes": {col: str(dtype) for col, dtype in data.dtypes.items()},
            "missing_values": data.isnull().sum().to_dict(),
            "numeric_stats": data.describe().to_dict() if len(data.select_dtypes(include=["number"]).columns) > 0 else {},
            "categorical_stats": {}
        }
        
        # Add categorical statistics
        categorical_cols = data.select_dtypes(include=["object", "category"]).columns
        for col in categorical_cols:
            stats["categorical_stats"][col] = {
                "unique_count": data[col].nunique(),
                "top_values": data[col].value_counts().head(10).to_dict()
            }
        
        return stats



